{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPgwF+sXmJA9XhtmZW7AY6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sangh0/DeepLearning-Tutorial/blob/main/current_materials/3_cnn_with_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN (Convolutional Neural Network) 모델을 구현하고 MNIST hand-written dataset으로 학습하기\n",
        "\n",
        "- CNN은 DNN or MLP의 단점을 타파한 신경망이예요\n",
        "- 앞에 강의했던 DNN과 다른 점은 딱 하나예요 모델.\n",
        "- DNN은 fully connected layer의 연속이라면 CNN은 convolution layer로 이루어져 있어요"
      ],
      "metadata": {
        "id": "DaENF3HBP8Gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. 패키지 모듈 및 임포트"
      ],
      "metadata": {
        "id": "vrr7u6q9QXbz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5WMgvyjP6jI"
      },
      "outputs": [],
      "source": [
        "import numpy as np # 텐서 계산을 위해\n",
        "import matplotlib.pyplot as plt # 시각화를 위해\n",
        "\n",
        "import torch # 파이토치 텐서 사용을 위해\n",
        "import torch.nn as nn # 뉴럴 네트워크 빌드를 위해\n",
        "import torch.optim as optim # optimizer 사용을 위해\n",
        "import torchvision.datasets as dsets # torchvision에 내장된 MNIST 데이터셋 다운로드 위해\n",
        "import torchvision.transforms as transforms # torchvision 전처리를 위해\n",
        "from torch.utils.data import DataLoader # 딥러닝 학습 데이터로더 구현을 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. 하이퍼파라미터 설정"
      ],
      "metadata": {
        "id": "rXrKxysdQdTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "Config = {\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 1e-3,\n",
        "    'epochs': 10,\n",
        "}"
      ],
      "metadata": {
        "id": "KY20p5CQQdcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. MNIST hand-written dataset 로드하기"
      ],
      "metadata": {
        "id": "2kmFv4k2Qg3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "train_set = dsets.MNIST(\n",
        "    root='mnist/',\n",
        "    train=True,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True,\n",
        ")\n",
        "\n",
        "test_set = dsets.MNIST(\n",
        "    root='mnist/',\n",
        "    train=False,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=Config['batch_size'],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_set,\n",
        "    batch_size=Config['batch_size'],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MqcDVFEQhA0",
        "outputId": "c34fd862-df5e-475b-9400-77d270166169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 78284623.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 77551660.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 20761085.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14929881.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 여기까지는 DNN 부분과 똑같아요\n",
        "- 모델을 빌드해야 하는데 convolution layer로 쌓을게요\n",
        "- 그 전에 CNN을 구현하기 위해 알아야 하는 몇 가지가 있어요"
      ],
      "metadata": {
        "id": "ZC55h1nuQmAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN을 다루기 위해 필요한 용어들**\n",
        "- 1. convolution\n",
        "- 2. channel\n",
        "- 3. filter or kernel\n",
        "- 4. stride\n",
        "- 5. padding\n",
        "- 6. pooling\n",
        "\n",
        "\n",
        "### 1. Convolution\n",
        "\n",
        "<img src = \"https://camo.githubusercontent.com/f3959a36c49ab55f45e2fae1793757e1941eec2ebb1369b3ff025f4a96f88d94/687474703a2f2f646565706c6561726e696e672e7374616e666f72642e6564752f77696b692f696d616765732f362f36632f436f6e766f6c7574696f6e5f736368656d617469632e676966\">\n",
        "\n",
        "- convolution은 다음과 같이 수행이 돼요\n",
        "- 5x5의 초록색 박스인 feature map과 3x3의 노란색 박스에 해당하는 sliding window가 있어요\n",
        "- sliding window가 초록색 박스를 하나씩 훑으면서 새로운 feature map을 생성해요\n",
        "\n",
        "### 2. channel\n",
        "\n",
        "<img src = \"https://camo.githubusercontent.com/c61ee1746d97812e0b64f60bf8288c8bcdd209148dcde1b9eb18ebe3a5dfa01d/68747470733a2f2f74616577616e6d657265706f2e6769746875622e696f2f323031382f30312f636e6e2f6368616e6e656c2e6a7067\" width = 800>\n",
        "\n",
        "- 컬러 사진은 RGB 3개의 채널로 이루어져 있어요\n",
        "- 또한 convolution layer를 쌓으면서 가로 세로 사이즈는 작아지고 채널 갯수는 많아져요\n",
        "- 즉, 앞 부분 레이어는 이미지 디테일을 파악하기 위해 존재하고\n",
        "- 뒷 부분 레이어는 전체적인 컨텍스트를 파악하기 위해 존재한다고 생각하시면 됩니다\n",
        "- 왜 그런건지는 이따 밑에서 보시죠\n",
        "\n",
        "### 3. filter or kernel\n",
        "\n",
        "<img src = \"https://camo.githubusercontent.com/f2ea5e053843a1c7198a74bbebea914b9adb2e8446c52e240e68a48cfdf15140/68747470733a2f2f74616577616e6d657265706f2e6769746875622e696f2f323031382f30312f636e6e2f636f6e762e706e67\">\n",
        "\n",
        "- 이미지의 특징을 찾아내는 역할을 수행해요\n",
        "- 얘네들이 결국 CNN의 파라미터에 해당해요\n",
        "- 즉, 학습의 대상이 되는거죠\n",
        "- filter는 지정된 간격(stride)으로 이동하면서 이미지와 합성하면서 feature map을 만들어내요\n",
        "\n",
        "### 4. stride\n",
        "\n",
        "<img src = \"https://camo.githubusercontent.com/b85b2ab96ef08b113a01b4d5476f8dd1af3ce39665ca4194d0bc5f9b943e4228/68747470733a2f2f74616577616e6d657265706f2e6769746875622e696f2f323031382f30312f636e6e2f66696c7465722e6a7067\">\n",
        "\n",
        "- filter를 얼만큼 순회할지 결정해요\n",
        "- stride=1로 설정하면 1칸씩 이동하면서 합성곱을 수행해요\n",
        "\n",
        "\n",
        "### 5. padding\n",
        "\n",
        "<img src = \"https://camo.githubusercontent.com/752b077999e432c520bb95c51b38c98eafc507289760d42d48d6c555eeec0ed5/68747470733a2f2f74616577616e6d657265706f2e6769746875622e696f2f323031382f30312f636e6e2f70616464696e672e706e67\">\n",
        "\n",
        "- convolution layer에서 stride로 인해 feature map의 크기는 input image보다 크기가 작아요\n",
        "- output의 크기가 줄어드는 것을 방지하는 것이 padding입니다\n",
        "- padding은 외곽에 지정된 픽셀만큼 특정 값으로 채운다는 것을 의미해요\n",
        "- 보통 0으로 많이 채워요\n",
        "\n",
        "\n",
        "### 6. pooling\n",
        "\n",
        "<img src = \"https://camo.githubusercontent.com/3952a493704e8e9582c4e86a3c075bb2936e2a6a1c11c5551ee2b00631fe7fa5/68747470733a2f2f74616577616e6d657265706f2e6769746875622e696f2f323031382f30322f636e6e2f6d617870756c6c696e672e706e67\">\n",
        "\n",
        "- pooling은 convolution layer의 출력을 입력으로 받아 크기를 줄여줘요\n",
        "- 또는 특정 feature를 강조하는 용도로 사용되기도 해요\n",
        "- max pooling, average pooling 등이 존재해요"
      ],
      "metadata": {
        "id": "LcP717_pQy0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. CNN 모델 빌드하기"
      ],
      "metadata": {
        "id": "tq-JvZAxSyoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim=1, hidden_dim=8, out_dim=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, hidden_dim, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(hidden_dim, hidden_dim*2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(7*7*16, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.features(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(CNN(), (1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2bQKO7TQmIL",
        "outputId": "219f65c6-14be-4a8b-c0fe-05b84cc316c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              80\n",
            "              ReLU-2            [-1, 8, 28, 28]               0\n",
            "         MaxPool2d-3            [-1, 8, 14, 14]               0\n",
            "            Conv2d-4           [-1, 16, 14, 14]           1,168\n",
            "              ReLU-5           [-1, 16, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 16, 7, 7]               0\n",
            "            Linear-7                  [-1, 100]          78,500\n",
            "              ReLU-8                  [-1, 100]               0\n",
            "            Linear-9                   [-1, 10]           1,010\n",
            "================================================================\n",
            "Total params: 80,758\n",
            "Trainable params: 80,758\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.16\n",
            "Params size (MB): 0.31\n",
            "Estimated Total Size (MB): 0.47\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. 모델 학습"
      ],
      "metadata": {
        "id": "FVvNO8FiTCMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = CNN().to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=Config['learning_rate'])\n",
        "\n",
        "def cal_accuracy(outputs, labels):\n",
        "    outputs = torch.argmax(outputs, dim=1)\n",
        "    correct = (outputs == labels).sum()/len(outputs)\n",
        "    return correct\n",
        "\n",
        "\n",
        "# Training\n",
        "for epoch in range(Config['epochs']):\n",
        "    for batch, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        acc = cal_accuracy(outputs, labels)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch+1) % 100 == 0:\n",
        "            print(f'Epoch {epoch+1}/{Config[\"epochs\"]}, Batch {batch+1}/{len(train_loader)}\\n'\n",
        "                  f'loss: {loss.item():.3f}, accuracy: {acc.item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9L9ElPOS7IK",
        "outputId": "e18401b3-8078-4981-9693-c72c7b9dc761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Batch 100/1875\n",
            "loss: 0.488, accuracy: 0.844\n",
            "Epoch 1/10, Batch 200/1875\n",
            "loss: 0.284, accuracy: 0.938\n",
            "Epoch 1/10, Batch 300/1875\n",
            "loss: 0.250, accuracy: 0.906\n",
            "Epoch 1/10, Batch 400/1875\n",
            "loss: 0.230, accuracy: 0.938\n",
            "Epoch 1/10, Batch 500/1875\n",
            "loss: 0.098, accuracy: 0.969\n",
            "Epoch 1/10, Batch 600/1875\n",
            "loss: 0.346, accuracy: 0.875\n",
            "Epoch 1/10, Batch 700/1875\n",
            "loss: 0.130, accuracy: 0.969\n",
            "Epoch 1/10, Batch 800/1875\n",
            "loss: 0.078, accuracy: 0.969\n",
            "Epoch 1/10, Batch 900/1875\n",
            "loss: 0.113, accuracy: 0.938\n",
            "Epoch 1/10, Batch 1000/1875\n",
            "loss: 0.154, accuracy: 0.938\n",
            "Epoch 1/10, Batch 1100/1875\n",
            "loss: 0.064, accuracy: 0.969\n",
            "Epoch 1/10, Batch 1200/1875\n",
            "loss: 0.065, accuracy: 0.969\n",
            "Epoch 1/10, Batch 1300/1875\n",
            "loss: 0.101, accuracy: 0.969\n",
            "Epoch 1/10, Batch 1400/1875\n",
            "loss: 0.086, accuracy: 0.969\n",
            "Epoch 1/10, Batch 1500/1875\n",
            "loss: 0.129, accuracy: 0.969\n",
            "Epoch 1/10, Batch 1600/1875\n",
            "loss: 0.273, accuracy: 0.938\n",
            "Epoch 1/10, Batch 1700/1875\n",
            "loss: 0.041, accuracy: 1.000\n",
            "Epoch 1/10, Batch 1800/1875\n",
            "loss: 0.128, accuracy: 0.938\n",
            "Epoch 2/10, Batch 100/1875\n",
            "loss: 0.012, accuracy: 1.000\n",
            "Epoch 2/10, Batch 200/1875\n",
            "loss: 0.056, accuracy: 0.969\n",
            "Epoch 2/10, Batch 300/1875\n",
            "loss: 0.157, accuracy: 0.906\n",
            "Epoch 2/10, Batch 400/1875\n",
            "loss: 0.013, accuracy: 1.000\n",
            "Epoch 2/10, Batch 500/1875\n",
            "loss: 0.106, accuracy: 0.969\n",
            "Epoch 2/10, Batch 600/1875\n",
            "loss: 0.038, accuracy: 1.000\n",
            "Epoch 2/10, Batch 700/1875\n",
            "loss: 0.042, accuracy: 1.000\n",
            "Epoch 2/10, Batch 800/1875\n",
            "loss: 0.267, accuracy: 0.906\n",
            "Epoch 2/10, Batch 900/1875\n",
            "loss: 0.017, accuracy: 1.000\n",
            "Epoch 2/10, Batch 1000/1875\n",
            "loss: 0.005, accuracy: 1.000\n",
            "Epoch 2/10, Batch 1100/1875\n",
            "loss: 0.046, accuracy: 1.000\n",
            "Epoch 2/10, Batch 1200/1875\n",
            "loss: 0.230, accuracy: 0.969\n",
            "Epoch 2/10, Batch 1300/1875\n",
            "loss: 0.006, accuracy: 1.000\n",
            "Epoch 2/10, Batch 1400/1875\n",
            "loss: 0.114, accuracy: 0.969\n",
            "Epoch 2/10, Batch 1500/1875\n",
            "loss: 0.057, accuracy: 0.969\n",
            "Epoch 2/10, Batch 1600/1875\n",
            "loss: 0.140, accuracy: 0.969\n",
            "Epoch 2/10, Batch 1700/1875\n",
            "loss: 0.028, accuracy: 1.000\n",
            "Epoch 2/10, Batch 1800/1875\n",
            "loss: 0.016, accuracy: 1.000\n",
            "Epoch 3/10, Batch 100/1875\n",
            "loss: 0.026, accuracy: 1.000\n",
            "Epoch 3/10, Batch 200/1875\n",
            "loss: 0.188, accuracy: 0.938\n",
            "Epoch 3/10, Batch 300/1875\n",
            "loss: 0.102, accuracy: 0.938\n",
            "Epoch 3/10, Batch 400/1875\n",
            "loss: 0.010, accuracy: 1.000\n",
            "Epoch 3/10, Batch 500/1875\n",
            "loss: 0.005, accuracy: 1.000\n",
            "Epoch 3/10, Batch 600/1875\n",
            "loss: 0.122, accuracy: 0.938\n",
            "Epoch 3/10, Batch 700/1875\n",
            "loss: 0.023, accuracy: 1.000\n",
            "Epoch 3/10, Batch 800/1875\n",
            "loss: 0.012, accuracy: 1.000\n",
            "Epoch 3/10, Batch 900/1875\n",
            "loss: 0.012, accuracy: 1.000\n",
            "Epoch 3/10, Batch 1000/1875\n",
            "loss: 0.004, accuracy: 1.000\n",
            "Epoch 3/10, Batch 1100/1875\n",
            "loss: 0.061, accuracy: 0.969\n",
            "Epoch 3/10, Batch 1200/1875\n",
            "loss: 0.043, accuracy: 0.969\n",
            "Epoch 3/10, Batch 1300/1875\n",
            "loss: 0.039, accuracy: 0.969\n",
            "Epoch 3/10, Batch 1400/1875\n",
            "loss: 0.077, accuracy: 0.938\n",
            "Epoch 3/10, Batch 1500/1875\n",
            "loss: 0.022, accuracy: 1.000\n",
            "Epoch 3/10, Batch 1600/1875\n",
            "loss: 0.013, accuracy: 1.000\n",
            "Epoch 3/10, Batch 1700/1875\n",
            "loss: 0.011, accuracy: 1.000\n",
            "Epoch 3/10, Batch 1800/1875\n",
            "loss: 0.014, accuracy: 1.000\n",
            "Epoch 4/10, Batch 100/1875\n",
            "loss: 0.017, accuracy: 1.000\n",
            "Epoch 4/10, Batch 200/1875\n",
            "loss: 0.041, accuracy: 0.969\n",
            "Epoch 4/10, Batch 300/1875\n",
            "loss: 0.077, accuracy: 0.969\n",
            "Epoch 4/10, Batch 400/1875\n",
            "loss: 0.048, accuracy: 0.969\n",
            "Epoch 4/10, Batch 500/1875\n",
            "loss: 0.030, accuracy: 1.000\n",
            "Epoch 4/10, Batch 600/1875\n",
            "loss: 0.006, accuracy: 1.000\n",
            "Epoch 4/10, Batch 700/1875\n",
            "loss: 0.025, accuracy: 1.000\n",
            "Epoch 4/10, Batch 800/1875\n",
            "loss: 0.023, accuracy: 1.000\n",
            "Epoch 4/10, Batch 900/1875\n",
            "loss: 0.019, accuracy: 1.000\n",
            "Epoch 4/10, Batch 1000/1875\n",
            "loss: 0.017, accuracy: 1.000\n",
            "Epoch 4/10, Batch 1100/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 4/10, Batch 1200/1875\n",
            "loss: 0.034, accuracy: 1.000\n",
            "Epoch 4/10, Batch 1300/1875\n",
            "loss: 0.017, accuracy: 1.000\n",
            "Epoch 4/10, Batch 1400/1875\n",
            "loss: 0.004, accuracy: 1.000\n",
            "Epoch 4/10, Batch 1500/1875\n",
            "loss: 0.010, accuracy: 1.000\n",
            "Epoch 4/10, Batch 1600/1875\n",
            "loss: 0.004, accuracy: 1.000\n",
            "Epoch 4/10, Batch 1700/1875\n",
            "loss: 0.002, accuracy: 1.000\n",
            "Epoch 4/10, Batch 1800/1875\n",
            "loss: 0.018, accuracy: 1.000\n",
            "Epoch 5/10, Batch 100/1875\n",
            "loss: 0.023, accuracy: 1.000\n",
            "Epoch 5/10, Batch 200/1875\n",
            "loss: 0.077, accuracy: 0.938\n",
            "Epoch 5/10, Batch 300/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 5/10, Batch 400/1875\n",
            "loss: 0.010, accuracy: 1.000\n",
            "Epoch 5/10, Batch 500/1875\n",
            "loss: 0.076, accuracy: 0.969\n",
            "Epoch 5/10, Batch 600/1875\n",
            "loss: 0.024, accuracy: 1.000\n",
            "Epoch 5/10, Batch 700/1875\n",
            "loss: 0.009, accuracy: 1.000\n",
            "Epoch 5/10, Batch 800/1875\n",
            "loss: 0.012, accuracy: 1.000\n",
            "Epoch 5/10, Batch 900/1875\n",
            "loss: 0.002, accuracy: 1.000\n",
            "Epoch 5/10, Batch 1000/1875\n",
            "loss: 0.072, accuracy: 0.938\n",
            "Epoch 5/10, Batch 1100/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 5/10, Batch 1200/1875\n",
            "loss: 0.002, accuracy: 1.000\n",
            "Epoch 5/10, Batch 1300/1875\n",
            "loss: 0.018, accuracy: 1.000\n",
            "Epoch 5/10, Batch 1400/1875\n",
            "loss: 0.011, accuracy: 1.000\n",
            "Epoch 5/10, Batch 1500/1875\n",
            "loss: 0.030, accuracy: 1.000\n",
            "Epoch 5/10, Batch 1600/1875\n",
            "loss: 0.003, accuracy: 1.000\n",
            "Epoch 5/10, Batch 1700/1875\n",
            "loss: 0.006, accuracy: 1.000\n",
            "Epoch 5/10, Batch 1800/1875\n",
            "loss: 0.015, accuracy: 1.000\n",
            "Epoch 6/10, Batch 100/1875\n",
            "loss: 0.011, accuracy: 1.000\n",
            "Epoch 6/10, Batch 200/1875\n",
            "loss: 0.007, accuracy: 1.000\n",
            "Epoch 6/10, Batch 300/1875\n",
            "loss: 0.002, accuracy: 1.000\n",
            "Epoch 6/10, Batch 400/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 6/10, Batch 500/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 6/10, Batch 600/1875\n",
            "loss: 0.010, accuracy: 1.000\n",
            "Epoch 6/10, Batch 700/1875\n",
            "loss: 0.006, accuracy: 1.000\n",
            "Epoch 6/10, Batch 800/1875\n",
            "loss: 0.023, accuracy: 1.000\n",
            "Epoch 6/10, Batch 900/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 6/10, Batch 1000/1875\n",
            "loss: 0.080, accuracy: 0.969\n",
            "Epoch 6/10, Batch 1100/1875\n",
            "loss: 0.056, accuracy: 0.969\n",
            "Epoch 6/10, Batch 1200/1875\n",
            "loss: 0.002, accuracy: 1.000\n",
            "Epoch 6/10, Batch 1300/1875\n",
            "loss: 0.003, accuracy: 1.000\n",
            "Epoch 6/10, Batch 1400/1875\n",
            "loss: 0.028, accuracy: 1.000\n",
            "Epoch 6/10, Batch 1500/1875\n",
            "loss: 0.012, accuracy: 1.000\n",
            "Epoch 6/10, Batch 1600/1875\n",
            "loss: 0.028, accuracy: 1.000\n",
            "Epoch 6/10, Batch 1700/1875\n",
            "loss: 0.004, accuracy: 1.000\n",
            "Epoch 6/10, Batch 1800/1875\n",
            "loss: 0.125, accuracy: 0.969\n",
            "Epoch 7/10, Batch 100/1875\n",
            "loss: 0.006, accuracy: 1.000\n",
            "Epoch 7/10, Batch 200/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 7/10, Batch 300/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 7/10, Batch 400/1875\n",
            "loss: 0.002, accuracy: 1.000\n",
            "Epoch 7/10, Batch 500/1875\n",
            "loss: 0.046, accuracy: 0.969\n",
            "Epoch 7/10, Batch 600/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 7/10, Batch 700/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 7/10, Batch 800/1875\n",
            "loss: 0.002, accuracy: 1.000\n",
            "Epoch 7/10, Batch 900/1875\n",
            "loss: 0.014, accuracy: 1.000\n",
            "Epoch 7/10, Batch 1000/1875\n",
            "loss: 0.003, accuracy: 1.000\n",
            "Epoch 7/10, Batch 1100/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 7/10, Batch 1200/1875\n",
            "loss: 0.002, accuracy: 1.000\n",
            "Epoch 7/10, Batch 1300/1875\n",
            "loss: 0.011, accuracy: 1.000\n",
            "Epoch 7/10, Batch 1400/1875\n",
            "loss: 0.009, accuracy: 1.000\n",
            "Epoch 7/10, Batch 1500/1875\n",
            "loss: 0.003, accuracy: 1.000\n",
            "Epoch 7/10, Batch 1600/1875\n",
            "loss: 0.037, accuracy: 1.000\n",
            "Epoch 7/10, Batch 1700/1875\n",
            "loss: 0.003, accuracy: 1.000\n",
            "Epoch 7/10, Batch 1800/1875\n",
            "loss: 0.027, accuracy: 1.000\n",
            "Epoch 8/10, Batch 100/1875\n",
            "loss: 0.005, accuracy: 1.000\n",
            "Epoch 8/10, Batch 200/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 8/10, Batch 300/1875\n",
            "loss: 0.103, accuracy: 0.969\n",
            "Epoch 8/10, Batch 400/1875\n",
            "loss: 0.024, accuracy: 0.969\n",
            "Epoch 8/10, Batch 500/1875\n",
            "loss: 0.002, accuracy: 1.000\n",
            "Epoch 8/10, Batch 600/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 8/10, Batch 700/1875\n",
            "loss: 0.032, accuracy: 0.969\n",
            "Epoch 8/10, Batch 800/1875\n",
            "loss: 0.003, accuracy: 1.000\n",
            "Epoch 8/10, Batch 900/1875\n",
            "loss: 0.036, accuracy: 0.969\n",
            "Epoch 8/10, Batch 1000/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 8/10, Batch 1100/1875\n",
            "loss: 0.129, accuracy: 0.969\n",
            "Epoch 8/10, Batch 1200/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 8/10, Batch 1300/1875\n",
            "loss: 0.005, accuracy: 1.000\n",
            "Epoch 8/10, Batch 1400/1875\n",
            "loss: 0.031, accuracy: 0.969\n",
            "Epoch 8/10, Batch 1500/1875\n",
            "loss: 0.007, accuracy: 1.000\n",
            "Epoch 8/10, Batch 1600/1875\n",
            "loss: 0.036, accuracy: 0.969\n",
            "Epoch 8/10, Batch 1700/1875\n",
            "loss: 0.156, accuracy: 0.969\n",
            "Epoch 8/10, Batch 1800/1875\n",
            "loss: 0.249, accuracy: 0.906\n",
            "Epoch 9/10, Batch 100/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 9/10, Batch 200/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 9/10, Batch 300/1875\n",
            "loss: 0.061, accuracy: 0.969\n",
            "Epoch 9/10, Batch 400/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 9/10, Batch 500/1875\n",
            "loss: 0.017, accuracy: 1.000\n",
            "Epoch 9/10, Batch 600/1875\n",
            "loss: 0.006, accuracy: 1.000\n",
            "Epoch 9/10, Batch 700/1875\n",
            "loss: 0.003, accuracy: 1.000\n",
            "Epoch 9/10, Batch 800/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 9/10, Batch 900/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 9/10, Batch 1000/1875\n",
            "loss: 0.008, accuracy: 1.000\n",
            "Epoch 9/10, Batch 1100/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 9/10, Batch 1200/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 9/10, Batch 1300/1875\n",
            "loss: 0.012, accuracy: 1.000\n",
            "Epoch 9/10, Batch 1400/1875\n",
            "loss: 0.003, accuracy: 1.000\n",
            "Epoch 9/10, Batch 1500/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 9/10, Batch 1600/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 9/10, Batch 1700/1875\n",
            "loss: 0.018, accuracy: 1.000\n",
            "Epoch 9/10, Batch 1800/1875\n",
            "loss: 0.018, accuracy: 1.000\n",
            "Epoch 10/10, Batch 100/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 10/10, Batch 200/1875\n",
            "loss: 0.133, accuracy: 0.969\n",
            "Epoch 10/10, Batch 300/1875\n",
            "loss: 0.005, accuracy: 1.000\n",
            "Epoch 10/10, Batch 400/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 10/10, Batch 500/1875\n",
            "loss: 0.004, accuracy: 1.000\n",
            "Epoch 10/10, Batch 600/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 10/10, Batch 700/1875\n",
            "loss: 0.034, accuracy: 1.000\n",
            "Epoch 10/10, Batch 800/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 10/10, Batch 900/1875\n",
            "loss: 0.001, accuracy: 1.000\n",
            "Epoch 10/10, Batch 1000/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 10/10, Batch 1100/1875\n",
            "loss: 0.045, accuracy: 0.969\n",
            "Epoch 10/10, Batch 1200/1875\n",
            "loss: 0.008, accuracy: 1.000\n",
            "Epoch 10/10, Batch 1300/1875\n",
            "loss: 0.003, accuracy: 1.000\n",
            "Epoch 10/10, Batch 1400/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 10/10, Batch 1500/1875\n",
            "loss: 0.000, accuracy: 1.000\n",
            "Epoch 10/10, Batch 1600/1875\n",
            "loss: 0.004, accuracy: 1.000\n",
            "Epoch 10/10, Batch 1700/1875\n",
            "loss: 0.016, accuracy: 1.000\n",
            "Epoch 10/10, Batch 1800/1875\n",
            "loss: 0.000, accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6. 모델 성능 평가"
      ],
      "metadata": {
        "id": "T67GAmwXTNsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "test_loss, test_acc = 0, 0\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for batch, (images, labels) in enumerate(test_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        acc = cal_accuracy(outputs, labels)\n",
        "        test_acc += acc.item()\n",
        "        loss = loss_func(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "print(f'Test Loss: {test_loss/(batch+1):.3f}, Test Accuracy: {test_acc/(batch+1):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5vC2xTnTI__",
        "outputId": "1a2569dd-c80c-427a-d8eb-2c14aeb99ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.043, Test Accuracy: 0.988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVgs8OAOTQ5s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}